{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collected-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course DSC 650 - Data Mining\n",
    "# Name - Vikas Ranjan\n",
    "# Assignment - Assignment 11 - Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-wagner",
   "metadata": {},
   "source": [
    "### Text generation with LSTM \n",
    "Since we need is a lot of text data that we can use to learn a language model. You could use any suﬀiciently large text file or set of text files – Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "focused-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "keras.__version__\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import random\n",
    "import sys\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-finding",
   "metadata": {},
   "source": [
    "Download the nietzsche file and convert the contents to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modern-sheep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 0s 1us/step\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-blake",
   "metadata": {},
   "source": [
    "Extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array x of shape (sequences, maxlen, unique_characters). Simultane- ously, we prepare a array y containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "active-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step): \n",
    "    sentences.append(text[i: i + maxlen]) \n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "# Dictionary mapping unique characters to their index in `chars` \n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) \n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence): \n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-rachel",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. But let us note that recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personal-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-handle",
   "metadata": {},
   "source": [
    "### Training the language model and sampling from it\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "2) Reweighting the distribution to a certain “temperature”\n",
    "3) Sampling the next character at random according to the reweighted distribution\n",
    "4) Adding the new character at the end of the available text\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, and draw a character index from it (the “sampling function”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generic-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64') \n",
    "    preds = np.log(preds) / temperature \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-position",
   "metadata": {},
   "source": [
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "leading-patrol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "1565/1565 [==============================] - 199s 125ms/step - loss: 2.2675\n",
      "--- Generating with seed: \"ious\n",
      "conscience, that the self-reliance of the community is \"\n",
      "------ temperature: 0.2\n",
      "ious\n",
      "conscience, that the self-reliance of the community is the ore in the man in the mest and sould been an in the read and in the really of the experience of the reading that who is the experience of the most than the highest than the most and in the most than the most than the experience of the experience of the most and all the man and are that which is the most the for the reading and be sould the most than the most that the philosophy that the experi\n",
      "------ temperature: 0.5\n",
      "d the most than the most that the philosophy that the experience, when in the denerally than the ens that is dessent the power in the more that is of man of the sill and or should the experience, the readous that the possible that the thright of the viel, world be in the explaments that is the man of the and the be\n",
      "ording of him that it deed in intereal of the comselves and have man persore of the respeltion of a could than, and the experience, than no mor\n",
      "------ temperature: 1.0\n",
      " respeltion of a could than, and the experience, than no mort to the very are of vill bult\" it were hoabtian at the\n",
      "infleral and enverulast ratilem. the nothing (what is not that that\n",
      "disturcaligared real, temed distrust, the self readed olagen understand been boven the cience than hav, and es has mustianter mand. it he barting be, the pholon to the onerent a dustlory deablicace to truth pate growduyoprations\n",
      "bfaitiodiantificusse of \"agrow, possetine--with\n",
      "------ temperature: 1.2\n",
      "duyoprations\n",
      "bfaitiodiantificusse of \"agrow, possetine--withse a\n",
      "movire musp, ourable goad; in conessebut\n",
      "af thoush, for-lenauncy\n",
      "fremth other in loghmenary\n",
      "exort, howeolument of a what\n",
      "elchise uspell is\n",
      "than, but tho scelfunaricismenciils with chull very could teskably deou freidness expasing conterdauss:\" howodd-ourtafes toocchisthils perday then\n",
      "havin insuscituiesly pethy aoveryoul-in no\n",
      "relitionablene.\n",
      "to neme? greated tewer: with which like do new, he\n",
      "epoch 2\n",
      "1565/1565 [==============================] - 194s 124ms/step - loss: 1.6134\n",
      "--- Generating with seed: \" of some few\n",
      "philosophers, men have placed sympathy very low\"\n",
      "------ temperature: 0.2\n",
      " of some few\n",
      "philosophers, men have placed sympathy very lower and have an are the present and the such and the have himself the sens the string of the sens and be interest and different and something of the such a man and the sent the there is the order that the sension of the sense of the sent the same the string his community of the man and the sens and the super of the sense of the such as a man be are the and self be it is the order the same something\n",
      "------ temperature: 0.5\n",
      "an be are the and self be it is the order the same something in all in only there is in a soul at the world even the\n",
      "preversent and sufferently or believe in the sens it was all the man and the very sufferently may be stepuding the there as a masters  that world the south and become with a man man in historical man interest in the matiles of the stupious man who one who one or again the complation of the though the of the\n",
      "communificed, the such a condition\n",
      "------ temperature: 1.0\n",
      " of the though the of the\n",
      "communificed, the such a conditionabed fact's profible our nay of being degrate--as orner. the dill 4ing that so\n",
      "are,, but the othernones of he hand--\n",
      "aybise onurg. not to\n",
      "philosophers of areagainsh, for aditated.\" even where a the is gerrartany and \"thoods and\n",
      "havese pluhed, and it have sebric of hervertry bean and our pe,  fuct had inmons flomro previsely, which our erry you, not toatent in iverginal does who becies become cour \n",
      "------ temperature: 1.2\n",
      "y you, not toatent in iverginal does who becies become cour immouth longenosed\n",
      "by homs possedsr, but the sind a, let\n",
      "are boded even and\n",
      "perhaps, and\n",
      "merely inforecom\n",
      "become natimed. and epritused \"not is\" begearl lihe hangings bochhonism the twaints. ie susplerusm case is gormermandhes; but will, aw, inst matners,lll!k\n",
      "nongereds of arl-civerl,\n",
      "and a dorbalitied as and not\n",
      "minds or his the\n",
      "nayl\n",
      "upon a\n",
      "previaring of. all are therewordsltifties, ih it\n",
      "dosy th\n",
      "epoch 3\n",
      "1565/1565 [==============================] - 195s 124ms/step - loss: 1.5264\n",
      "--- Generating with seed: \"evation. in germany,\n",
      "however (until quite recently when a ki\"\n",
      "------ temperature: 0.2\n",
      "evation. in germany,\n",
      "however (until quite recently when a kind to something and in the soul and with the self discornal the soul and because and and interromant for the sense to be the securation and because and in the same former the self--and the self-self to the something of the sense to say and seems to the self--the the sign and pleasing of the fact and self--the former the art to be an art and specially and constant to be an anter and and all the sel\n",
      "------ temperature: 0.5\n",
      "nd specially and constant to be an anter and and all the self-side and the thing and in their sense and because of the some himself to the religious methongers of the the highest also fact and pleasing of the precaustive and because of the warder to the serve to and be prevines to such a wally the returation and because of the last that the will to and the self--and of which is i word is a words and we is to make any one begation as that where is also are \n",
      "------ temperature: 1.0\n",
      "nd we is to make any one begation as that where is also are be a ne\"creation of the ofit of the brethant,\n",
      "she exacters. whecemseles to be will definiou unduck have coarse, and almost have without interited, because\n",
      "entiusion.sche he in there heart man perst\n",
      "phefinateriolity\n",
      "onet. law\n",
      "inmust bangely imby himself to\n",
      "himself.\n",
      "\n",
      "11b. the sufferen emennge to reary has men the excrecans believen has as i indubarianism in ewerent\n",
      "indeouss any \"and valuation that m\n",
      "------ temperature: 1.2\n",
      " indubarianism in ewerent\n",
      "indeouss any \"and valuation that menslocus oning their\n",
      "everelin spiritable larale: on the f.nal, whose of but\n",
      "he in the mae be\n",
      "man! this position many\n",
      "idenen: a\n",
      "domine co tever will a\n",
      "womdsocn? his distspan of a mankind frienss: if us-senate and and be!\n",
      "\n",
      "mod pures within\n",
      "in which believe scene, orapprise\n",
      "find, they is their propesse for0git, something bs the bir-meaparment. and schounds in his most stilo\n",
      "suring\n",
      "rsy, gor_-shere hav\n",
      "epoch 4\n",
      "1565/1565 [==============================] - 194s 124ms/step - loss: 1.4788\n",
      "--- Generating with seed: \"--my truths.\n",
      "\n",
      "232. woman wishes to be independent, and there\"\n",
      "------ temperature: 0.2\n",
      "--my truths.\n",
      "\n",
      "232. woman wishes to be independent, and there is not the sentiment of the sentiment of the same the same the sentiment of the consists of the consist and distinguished the sentiment of the sentiment of the standard and consists of the sentiment that it is the sentiment that the sentiment of the contemplation of the contemplation of the sentiment and distinguished and self-strenge of the consequently to the consequently of the profound of the\n",
      "------ temperature: 0.5\n",
      " the consequently to the consequently of the profound of the experration of his experiation of distingually for the sentictly and\n",
      "stronger and dissenting in the same experiation of the refined in the sentiment to the deteriation of the word and distinguished that it is not be an and god, there alove states of an out of the bast and been does and consequently for an experiation with other in the experiation and conditions in the content consequently even th\n",
      "------ temperature: 1.0\n",
      "periation and conditions in the content consequently even theory of the stach consist may his peilor onl's arten, have on. force; advactions, in which of warserby aponatically, in centance's because pray, however, will be taste, ors a seconding, him it\n",
      "assentual us eny, art of a most oney,  \"who do nature, back is not more called cased by thore conduct respabling, whole before siffer, of different in men\"-the fyer justifition of a small not very the\n",
      "same w\n",
      "------ temperature: 1.2\n",
      " in men\"-the fyer justifition of a small not very the\n",
      "same will not would not necess of their\n",
      "ventusment that\n",
      "un\n",
      "i couldsc the\n",
      "disse to hadterfleing of what belave one\n",
      ", other. without, be passion-aral beautared in\n",
      "someduded eait: too the chamed.lek\"--it sense, p.spealing:\n",
      "consists\n",
      "herage. han every new every, it impuscition of\n",
      "f ce. pinter other\n",
      "dlascity: anteerned they be judgg our ubterd, above emated, tolled us: seove\" by nimed and quitel\n",
      "nitust\n",
      "as a w\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data \n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "    \n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]: \n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars))) \n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "    \n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-graphics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
